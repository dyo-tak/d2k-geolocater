{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# task1. preprocessing tweets\n",
    "### load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweetsData = pd.read_csv(\"all_annotated.tsv\", sep = \"\\t\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet ID</th>\n",
       "      <th>Country</th>\n",
       "      <th>Date</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Definitely English</th>\n",
       "      <th>Ambiguous</th>\n",
       "      <th>Definitely Not English</th>\n",
       "      <th>Code-Switched</th>\n",
       "      <th>Ambiguous due to Named Entities</th>\n",
       "      <th>Automatically Generated Tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>434215992731136000</td>\n",
       "      <td>TR</td>\n",
       "      <td>2014-02-14</td>\n",
       "      <td>Bug√ºn bulusmami lazimdiii</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>285903159434563584</td>\n",
       "      <td>TR</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>Volkan konak adami tribe sokar yemin ederim :D</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>285948076496142336</td>\n",
       "      <td>NL</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>Bed</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>285965965118824448</td>\n",
       "      <td>US</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>I felt my first flash of violence at some fool...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>286057979831275520</td>\n",
       "      <td>US</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>Ladies drink and get in free till 10:30</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10497</th>\n",
       "      <td>774941788247298050</td>\n",
       "      <td>TR</td>\n",
       "      <td>2016-09-11</td>\n",
       "      <td>I'm at @PiazzaAvym in Canik, Samsun w/ @mertar...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10498</th>\n",
       "      <td>774951242422480897</td>\n",
       "      <td>PH</td>\n",
       "      <td>2016-09-11</td>\n",
       "      <td>El Nido, fica ao norte de Palawan, uma das ilh...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10499</th>\n",
       "      <td>774960083721531392</td>\n",
       "      <td>ID</td>\n",
       "      <td>2016-09-11</td>\n",
       "      <td>Alhamdulillah üòäüë®‚Äçüë©‚Äçüëß‚ÄçüëßüéÇüç¶makasih mah pah #lovyu...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10500</th>\n",
       "      <td>775057244798849024</td>\n",
       "      <td>NG</td>\n",
       "      <td>2016-09-11</td>\n",
       "      <td>Eid-Mubarak @ Bauchi Fedral Lowcost https://t....</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10501</th>\n",
       "      <td>775072793385766912</td>\n",
       "      <td>GT</td>\n",
       "      <td>2016-09-11</td>\n",
       "      <td>‚íç Santa Catarina Pinula\\n‚íé Nairo Quintana\\n‚íè A...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10502 rows √ó 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Tweet ID Country        Date  \\\n",
       "0      434215992731136000      TR  2014-02-14   \n",
       "1      285903159434563584      TR  2013-01-01   \n",
       "2      285948076496142336      NL  2013-01-01   \n",
       "3      285965965118824448      US  2013-01-01   \n",
       "4      286057979831275520      US  2013-01-01   \n",
       "...                   ...     ...         ...   \n",
       "10497  774941788247298050      TR  2016-09-11   \n",
       "10498  774951242422480897      PH  2016-09-11   \n",
       "10499  774960083721531392      ID  2016-09-11   \n",
       "10500  775057244798849024      NG  2016-09-11   \n",
       "10501  775072793385766912      GT  2016-09-11   \n",
       "\n",
       "                                                   Tweet  Definitely English  \\\n",
       "0                              Bug√ºn bulusmami lazimdiii                   0   \n",
       "1         Volkan konak adami tribe sokar yemin ederim :D                   0   \n",
       "2                                                    Bed                   1   \n",
       "3      I felt my first flash of violence at some fool...                   1   \n",
       "4                Ladies drink and get in free till 10:30                   1   \n",
       "...                                                  ...                 ...   \n",
       "10497  I'm at @PiazzaAvym in Canik, Samsun w/ @mertar...                   1   \n",
       "10498  El Nido, fica ao norte de Palawan, uma das ilh...                   0   \n",
       "10499  Alhamdulillah üòäüë®‚Äçüë©‚Äçüëß‚ÄçüëßüéÇüç¶makasih mah pah #lovyu...                   0   \n",
       "10500  Eid-Mubarak @ Bauchi Fedral Lowcost https://t....                   0   \n",
       "10501  ‚íç Santa Catarina Pinula\\n‚íé Nairo Quintana\\n‚íè A...                   0   \n",
       "\n",
       "       Ambiguous  Definitely Not English  Code-Switched  \\\n",
       "0              0                       1              0   \n",
       "1              0                       1              0   \n",
       "2              0                       0              0   \n",
       "3              0                       0              0   \n",
       "4              0                       0              0   \n",
       "...          ...                     ...            ...   \n",
       "10497          0                       0              0   \n",
       "10498          0                       1              0   \n",
       "10499          0                       1              0   \n",
       "10500          1                       0              0   \n",
       "10501          1                       0              0   \n",
       "\n",
       "       Ambiguous due to Named Entities  Automatically Generated Tweets  \n",
       "0                                    0                               0  \n",
       "1                                    0                               0  \n",
       "2                                    0                               0  \n",
       "3                                    0                               0  \n",
       "4                                    0                               0  \n",
       "...                                ...                             ...  \n",
       "10497                                0                               1  \n",
       "10498                                0                               0  \n",
       "10499                                0                               0  \n",
       "10500                                1                               0  \n",
       "10501                                1                               0  \n",
       "\n",
       "[10502 rows x 10 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweetsData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing for tweets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_countries  = list(tweetsData['Country'].value_counts()[:20].to_dict().keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweetsData = tweetsData[tweetsData['Country'].isin(remove_countries)]\n",
    "tweetsData.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Country\n",
       "US    2966\n",
       "BR    1195\n",
       "ID    1099\n",
       "TR     624\n",
       "JP     505\n",
       "GB     481\n",
       "MY     395\n",
       "ES     340\n",
       "AR     312\n",
       "FR     265\n",
       "PH     233\n",
       "MX     229\n",
       "TH     168\n",
       "RU     164\n",
       "CA     121\n",
       "IT      95\n",
       "CL      90\n",
       "NL      83\n",
       "ZA      67\n",
       "CO      62\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweetsData['Country'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tweetsData['Tweet'].values\n",
    "y = tweetsData['Country'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of preprocessed tweets = 9303\n",
      "Number of preprocessed labels = 9303\n",
      "\n",
      "Samples of preprocessed data:\n",
      "Country = TR \tTweet = {'bug√ºn': 1, 'bulusmami': 1, 'lazimdiii': 1}\n",
      "Country = TR \tTweet = {'volkan': 1, 'konak': 1, 'adami': 1, 'tribe': 1, 'sokar': 1, 'yemin': 1, 'ederim': 1, ':d': 1}\n",
      "Country = NL \tTweet = {'bed': 1}\n",
      "Country = US \tTweet = {'felt': 1, 'first': 1, 'flash': 1, 'violence': 1, 'fool': 2, 'bumped': 1, 'pity': 1}\n",
      "Country = US \tTweet = {'ladies': 1, 'drink': 1, 'get': 1, 'free': 1, 'till': 1}\n",
      "Country = NL \tTweet = {'@melanynijholtxo': 1, 'ahhahahahah': 1, 'dm': 1}\n",
      "Country = US \tTweet = {'fuck': 1}\n",
      "Country = GB \tTweet = {'watching': 1, '#miranda': 1, 'bbc': 1, '@mermhart': 1, 'u': 1, 'r': 1, 'hilarious': 1}\n",
      "Country = US \tTweet = {'shopping': 1, \"kohl's\": 1, 'http://t.co/i8zkqht9': 1}\n",
      "Country = MX \tTweet = {'@mizzh_': 1, 'celos': 1, 'es': 1, 'tu': 1, 'segundo': 1, 'nombre': 1}\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "tt = TweetTokenizer()\n",
    "stopwords = set(stopwords.words('english')) #note: stopwords are all in lowercase\n",
    "\n",
    "def preprocess_data(data, labels):\n",
    "    cleaned_tokens = []\n",
    "    cleaned_labels = []\n",
    "    \n",
    "    for string, label in zip(data, labels):\n",
    "        dic = {}\n",
    "        tokens = tt.tokenize(string)    # step1. tokenize a tweet data\n",
    "        for token in tokens:\n",
    "            token = token.lower()       # step2. lowercase words\n",
    "            \n",
    "            # step3 & step4. save words including any English alphabets and not belonging to stopwords \n",
    "            if (re.search(\"[a-z]+\",token) and not token in stopwords):   \n",
    "                dic[token] = dic.get(token,0) + 1  \n",
    "                \n",
    "        # save preprocessed tokens with their label if any token exists\n",
    "        if len(dic) > 0:\n",
    "            cleaned_tokens.append(dic)\n",
    "            cleaned_labels.append(label)\n",
    "    \n",
    "    return cleaned_tokens, cleaned_labels    \n",
    "\n",
    "x_processed, y_processed = preprocess_data(x, y)\n",
    "\n",
    "print(\"Number of preprocessed tweets =\", len(x_processed))\n",
    "print(\"Number of preprocessed labels =\", len(y_processed))\n",
    "print(\"\\nSamples of preprocessed data:\")\n",
    "for i in range(10):\n",
    "    print(\"Country =\", y_processed[i], \"\\tTweet =\", x_processed[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test block\n",
    "assert(len(x_processed) == len(y_processed))\n",
    "assert(len(x_processed) > 800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task2. text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### split datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# initialise the objects\n",
    "x_train, x_dev, x_test = None, None, None\n",
    "y_train, y_dev, y_test = None, None, None\n",
    "\n",
    "# split datasets into 70:15:15 with tha same distribution\n",
    "x_train, x_temp, y_train, y_temp = train_test_split(x_processed, y_processed, test_size = 0.3, \\\n",
    "                                                    stratify = y_processed, random_state = 1)\n",
    "x_dev, x_test, y_dev, y_test = train_test_split(x_temp, y_temp, test_size = 0.5, \\\n",
    "                                                stratify = y_temp, random_state = 1)\n",
    "\n",
    "# vectorize tokens \n",
    "vectorizer = DictVectorizer()\n",
    "x_train = vectorizer.fit_transform(x_train)\n",
    "x_dev = vectorizer.transform(x_dev)\n",
    "x_test = vectorizer.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes and Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<6512x23704 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 46917 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================\n",
      "MultinomialNB\n",
      "alpha 0.001 w/ accuracy 0.6652329749103942\n",
      "alpha 0.01 w/ accuracy 0.6551971326164875\n",
      "alpha 0.1 w/ accuracy 0.6587813620071684\n",
      "alpha 1.0 w/ accuracy 0.628673835125448\n",
      "alpha 10.0 w/ accuracy 0.5068100358422939\n",
      "alpha 100.0 w/ accuracy 0.374910394265233\n",
      "===========================\n",
      "LogisticRegression\n",
      "C 0.001 w/ accuracy 0.31684587813620074\n",
      "C 0.01 w/ accuracy 0.38853046594982077\n",
      "C 0.1 w/ accuracy 0.5189964157706093\n",
      "C 0.5 w/ accuracy 0.5870967741935483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C 1 w/ accuracy 0.5978494623655914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C 5 w/ accuracy 0.607168458781362\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C 10 w/ accuracy 0.603584229390681\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C 50 w/ accuracy 0.603584229390681\n",
      "C 100 w/ accuracy 0.6014336917562724\n",
      "\n",
      "\n",
      "best classifier of mutinomial naive bayes model : MultinomialNB(alpha=0.001) with accuracy (0.665)\n",
      "best classifier of logistic regression model : LogisticRegression(C=5) with accuracy (0.607)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# tune hyper-paramters for MultinomialNB \n",
    "best_acc_NB = 0    \n",
    "best_clfNB = None\n",
    "alphas = np.logspace(-3, 2, num = 6)  \n",
    "\n",
    "print(\"===========================\\nMultinomialNB\")\n",
    "for a in alphas:\n",
    "    clfNB = MultinomialNB(alpha = a)\n",
    "    clfNB.fit(x_train,y_train)\n",
    "    prediction_NB = clfNB.predict(x_dev)\n",
    "    acc_NB = accuracy_score(y_dev,prediction_NB)\n",
    "    print(\"alpha\", a, \"w/ accuracy\", acc_NB)\n",
    "    if acc_NB >= best_acc_NB:\n",
    "        best_acc_NB = acc_NB\n",
    "        best_clfNB = clfNB\n",
    "    \n",
    "# tune hyper-paramters for LogisticRegression\n",
    "best_acc_LR = 0      \n",
    "best_clfLR = None\n",
    "# solver = ['newton-cg', 'liblinear','lbfgs']   \n",
    "# penalty = ['none', 'l1', 'l2']\n",
    "C = [0.001, 0.01, 0.1, 0.5, 1, 5, 10, 50, 100]    \n",
    "\n",
    "print(\"===========================\\nLogisticRegression\")\n",
    "for c in C:\n",
    "    clfLR = LogisticRegression(C = c)\n",
    "    clfLR.fit(x_train,y_train)\n",
    "    prediction_LR = clfLR.predict(x_dev)\n",
    "    acc_LR = accuracy_score(y_dev,prediction_LR)\n",
    "    print(\"C\", c, \"w/ accuracy\", acc_LR)\n",
    "    if acc_LR >= best_acc_LR:\n",
    "        best_acc_LR = acc_LR\n",
    "        best_clfLR = clfLR\n",
    "\n",
    "# print the optimal hyperparameters\n",
    "print(\"\\n\\nbest classifier of mutinomial naive bayes model : %s with accuracy (%.3f)\" \\\n",
    "      % (best_clfNB,best_acc_NB))                \n",
    "print(\"best classifier of logistic regression model : %s with accuracy (%.3f)\" \\\n",
    "      % (best_clfLR, best_acc_LR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### performance evaluation with the best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">> MultinomialNB Model Results <<\n",
      "\n",
      "Accuracy : 0.65\n",
      "Macro Avg. F-score : 0.437\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          AR       0.41      0.49      0.45        47\n",
      "          BR       0.88      0.84      0.86       178\n",
      "          CA       0.00      0.00      0.00        18\n",
      "          CL       0.33      0.23      0.27        13\n",
      "          CO       0.00      0.00      0.00         9\n",
      "          ES       0.31      0.25      0.28        51\n",
      "          FR       0.92      0.56      0.70        39\n",
      "          GB       0.28      0.18      0.22        72\n",
      "          ID       0.83      0.74      0.78       165\n",
      "          IT       0.83      0.36      0.50        14\n",
      "          JP       0.61      0.44      0.51        64\n",
      "          MX       0.36      0.29      0.32        34\n",
      "          MY       0.70      0.47      0.57        59\n",
      "          NL       0.67      0.50      0.57        12\n",
      "          PH       0.61      0.49      0.54        35\n",
      "          RU       0.25      0.11      0.15        18\n",
      "          TH       0.60      0.43      0.50        21\n",
      "          TR       0.87      0.73      0.80        94\n",
      "          US       0.62      0.88      0.73       443\n",
      "          ZA       0.00      0.00      0.00        10\n",
      "\n",
      "    accuracy                           0.65      1396\n",
      "   macro avg       0.50      0.40      0.44      1396\n",
      "weighted avg       0.64      0.65      0.63      1396\n",
      "\n",
      "\n",
      ">> Linear Regression Model Results <<\n",
      "\n",
      "Accuracy : 0.593\n",
      "Macro Avg. F-score : 0.339\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          AR       0.43      0.34      0.38        47\n",
      "          BR       0.80      0.78      0.79       178\n",
      "          CA       0.00      0.00      0.00        18\n",
      "          CL       0.00      0.00      0.00        13\n",
      "          CO       0.00      0.00      0.00         9\n",
      "          ES       0.33      0.20      0.25        51\n",
      "          FR       0.75      0.46      0.57        39\n",
      "          GB       0.30      0.08      0.13        72\n",
      "          ID       0.78      0.64      0.70       165\n",
      "          IT       1.00      0.29      0.44        14\n",
      "          JP       0.40      0.44      0.42        64\n",
      "          MX       0.40      0.12      0.18        34\n",
      "          MY       0.96      0.37      0.54        59\n",
      "          NL       0.50      0.08      0.14        12\n",
      "          PH       0.85      0.31      0.46        35\n",
      "          RU       0.00      0.00      0.00        18\n",
      "          TH       0.86      0.29      0.43        21\n",
      "          TR       0.91      0.54      0.68        94\n",
      "          US       0.52      0.92      0.67       443\n",
      "          ZA       0.00      0.00      0.00        10\n",
      "\n",
      "    accuracy                           0.59      1396\n",
      "   macro avg       0.49      0.29      0.34      1396\n",
      "weighted avg       0.60      0.59      0.55      1396\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nAccording to the result, LR model performs better with test dataset\\n'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import library\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "\n",
    "# fit the training data into the best Naive Bayes classifier and predict the label of test data\n",
    "best_clfNB.fit(x_train,y_train)          \n",
    "nb_prediction = best_clfNB.predict(x_test)\n",
    "\n",
    "print(\"\\n>> MultinomialNB Model Results <<\\n\")\n",
    "print(\"Accuracy :\", round(accuracy_score(y_test,nb_prediction), 3))\n",
    "print(\"Macro Avg. F-score :\", round(f1_score(y_test,nb_prediction, average='macro'), 3))\n",
    "print(classification_report(y_test, nb_prediction))\n",
    "\n",
    "\n",
    "# fit the training data into the best Logistic Regression classifier and predict the label of test data\n",
    "best_clfLR.fit(x_train,y_train)\n",
    "lr_prediction = best_clfLR.predict(x_test)\n",
    "       \n",
    "print(\"Accuracy :\", round(accuracy_score(y_test,lr_prediction), 3))\n",
    "print(\"Macro Avg. F-score :\", round(f1_score(y_test,lr_prediction, average='macro'),3))\n",
    "print(classification_report(y_test, lr_prediction))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x23704 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 1 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
